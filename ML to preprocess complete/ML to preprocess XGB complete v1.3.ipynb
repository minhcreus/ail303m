{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>VW_30cm</th>\n",
       "      <th>VW_60cm</th>\n",
       "      <th>VW_90cm</th>\n",
       "      <th>VW_120cm</th>\n",
       "      <th>VW_150cm</th>\n",
       "      <th>T_30cm</th>\n",
       "      <th>T_60cm</th>\n",
       "      <th>T_90cm</th>\n",
       "      <th>T_120cm</th>\n",
       "      <th>T_150cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/19/2009</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.314</td>\n",
       "      <td>11.87</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.70</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/20/2009</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.320</td>\n",
       "      <td>11.22</td>\n",
       "      <td>10.09</td>\n",
       "      <td>8.75</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/21/2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/22/2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.10</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.06</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/23/2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.25</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.16</td>\n",
       "      <td>8.03</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location        Date  VW_30cm  VW_60cm  VW_90cm  VW_120cm  VW_150cm  T_30cm  \\\n",
       "0   CAF357  05/19/2009    0.299    0.321    0.359     0.331     0.314   11.87   \n",
       "1   CAF357  05/20/2009    0.301    0.325    0.363     0.335     0.320   11.22   \n",
       "2   CAF357  05/21/2009      NaN      NaN      NaN       NaN       NaN     NaN   \n",
       "3   CAF357  05/22/2009      NaN    0.328    0.368     0.342     0.327   12.10   \n",
       "4   CAF357  05/23/2009      NaN    0.329    0.369     0.343     0.327   12.25   \n",
       "\n",
       "   T_60cm  T_90cm  T_120cm  T_150cm  \n",
       "0   10.10    8.70     7.61     7.09  \n",
       "1   10.09    8.75     7.61     7.00  \n",
       "2     NaN     NaN      NaN      NaN  \n",
       "3   10.11    9.06     7.94     7.22  \n",
       "4   10.40    9.16     8.03     7.32  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('CAF357.txt', sep = '\\t', na_values= 'NA')\n",
    "first_valid_index = df.iloc[:, 2:].notna().any(axis=1).idxmax()\n",
    "df_trimmed_top = df.iloc[first_valid_index:].reset_index(drop=True)\n",
    "df_trimmed_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_valid_index = df_trimmed_top.iloc[:, 2:].notna().any(axis=1)[::-1].idxmax()\n",
    "df_final_trimmed = df_trimmed_top.iloc[:last_valid_index + 1].reset_index(drop=True)\n",
    "df_final_trimmed.to_csv('CAF357_trimmed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location      0\n",
       "Date          0\n",
       "VW_30cm     487\n",
       "VW_60cm     267\n",
       "VW_90cm     367\n",
       "VW_120cm    319\n",
       "VW_150cm    217\n",
       "T_30cm      474\n",
       "T_60cm      267\n",
       "T_90cm      367\n",
       "T_120cm     319\n",
       "T_150cm     217\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_trimmed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_trimmed.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows left after cleaning: 2586\n"
     ]
    }
   ],
   "source": [
    "num_rows_left = df_final_trimmed.shape[0]\n",
    "print(f\"Number of rows left after cleaning: {num_rows_left}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2586 entries, 0 to 2585\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Location  2586 non-null   object \n",
      " 1   Date      2586 non-null   object \n",
      " 2   VW_30cm   2099 non-null   float64\n",
      " 3   VW_60cm   2319 non-null   float64\n",
      " 4   VW_90cm   2219 non-null   float64\n",
      " 5   VW_120cm  2267 non-null   float64\n",
      " 6   VW_150cm  2369 non-null   float64\n",
      " 7   T_30cm    2112 non-null   float64\n",
      " 8   T_60cm    2319 non-null   float64\n",
      " 9   T_90cm    2219 non-null   float64\n",
      " 10  T_120cm   2267 non-null   float64\n",
      " 11  T_150cm   2369 non-null   float64\n",
      "dtypes: float64(10), object(2)\n",
      "memory usage: 242.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final_trimmed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('CAF357_trimmed_test.txt', sep='\\t')\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data = data.drop(columns=['Date'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Location'] = label_encoder.fit_transform(data['Location'])\n",
    "\n",
    "columns_to_fill = data.columns.difference(['VW_30cm'])\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].isna().any():\n",
    "        columns_to_fill_temp = data.columns.difference([column])\n",
    "        data[columns_to_fill_temp] = data[columns_to_fill_temp].fillna(data[columns_to_fill_temp].median())\n",
    "        \n",
    "        data_missing = data[data[column].isna()]\n",
    "        data_not_missing = data[~data[column].isna()]\n",
    "        \n",
    "        features = data.columns.difference([column])\n",
    "        X = data_not_missing[features]\n",
    "        y = data_not_missing[column]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        X_missing = data_missing[features]\n",
    "        predicted_values = model.predict(X_missing)\n",
    "        data.loc[data[column].isna(), column] = predicted_values\n",
    "\n",
    "data.to_csv('CAF357_cleaned_xgb_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>VW_30cm</th>\n",
       "      <th>VW_60cm</th>\n",
       "      <th>VW_90cm</th>\n",
       "      <th>VW_120cm</th>\n",
       "      <th>VW_150cm</th>\n",
       "      <th>T_30cm</th>\n",
       "      <th>T_60cm</th>\n",
       "      <th>T_90cm</th>\n",
       "      <th>T_120cm</th>\n",
       "      <th>T_150cm</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.314</td>\n",
       "      <td>11.8700</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.70</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.09</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.320</td>\n",
       "      <td>11.2200</td>\n",
       "      <td>10.09</td>\n",
       "      <td>8.75</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.00</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.253920</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.307</td>\n",
       "      <td>9.3255</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.62</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.80</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.277450</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.1000</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.06</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.22</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.2500</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.16</td>\n",
       "      <td>8.03</td>\n",
       "      <td>7.32</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location   VW_30cm  VW_60cm  VW_90cm  VW_120cm  VW_150cm   T_30cm  T_60cm  \\\n",
       "0         0  0.299000    0.321    0.359     0.331     0.314  11.8700   10.10   \n",
       "1         0  0.301000    0.325    0.363     0.335     0.320  11.2200   10.09   \n",
       "2         0  0.253920    0.282    0.362     0.277     0.307   9.3255    7.75   \n",
       "3         0  0.277450    0.328    0.368     0.342     0.327  12.1000   10.11   \n",
       "4         0  0.271642    0.329    0.369     0.343     0.327  12.2500   10.40   \n",
       "\n",
       "   T_90cm  T_120cm  T_150cm  Day  Month  Year  \n",
       "0    8.70     7.61     7.09   19      5  2009  \n",
       "1    8.75     7.61     7.00   20      5  2009  \n",
       "2    9.62     8.90     8.80   21      5  2009  \n",
       "3    9.06     7.94     7.22   22      5  2009  \n",
       "4    9.16     8.03     7.32   23      5  2009  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('CAF357_cleaned_xgb_complete.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location    0\n",
       "VW_30cm     0\n",
       "VW_60cm     0\n",
       "VW_90cm     0\n",
       "VW_120cm    0\n",
       "VW_150cm    0\n",
       "T_30cm      0\n",
       "T_60cm      0\n",
       "T_90cm      0\n",
       "T_120cm     0\n",
       "T_150cm     0\n",
       "Day         0\n",
       "Month       0\n",
       "Year        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>VW_30cm</th>\n",
       "      <th>VW_60cm</th>\n",
       "      <th>VW_90cm</th>\n",
       "      <th>VW_120cm</th>\n",
       "      <th>VW_150cm</th>\n",
       "      <th>T_30cm</th>\n",
       "      <th>T_60cm</th>\n",
       "      <th>T_90cm</th>\n",
       "      <th>T_120cm</th>\n",
       "      <th>T_150cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/19/2009</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.314</td>\n",
       "      <td>11.8700</td>\n",
       "      <td>10.10</td>\n",
       "      <td>8.70</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/20/2009</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.320</td>\n",
       "      <td>11.2200</td>\n",
       "      <td>10.09</td>\n",
       "      <td>8.75</td>\n",
       "      <td>7.61</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/21/2009</td>\n",
       "      <td>0.253920</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.307</td>\n",
       "      <td>9.3255</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.62</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/22/2009</td>\n",
       "      <td>0.277450</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.1000</td>\n",
       "      <td>10.11</td>\n",
       "      <td>9.06</td>\n",
       "      <td>7.94</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAF357</td>\n",
       "      <td>05/23/2009</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.327</td>\n",
       "      <td>12.2500</td>\n",
       "      <td>10.40</td>\n",
       "      <td>9.16</td>\n",
       "      <td>8.03</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location        Date   VW_30cm  VW_60cm  VW_90cm  VW_120cm  VW_150cm  \\\n",
       "0   CAF357  05/19/2009  0.299000    0.321    0.359     0.331     0.314   \n",
       "1   CAF357  05/20/2009  0.301000    0.325    0.363     0.335     0.320   \n",
       "2   CAF357  05/21/2009  0.253920    0.282    0.362     0.277     0.307   \n",
       "3   CAF357  05/22/2009  0.277450    0.328    0.368     0.342     0.327   \n",
       "4   CAF357  05/23/2009  0.271642    0.329    0.369     0.343     0.327   \n",
       "\n",
       "    T_30cm  T_60cm  T_90cm  T_120cm  T_150cm  \n",
       "0  11.8700   10.10    8.70     7.61     7.09  \n",
       "1  11.2200   10.09    8.75     7.61     7.00  \n",
       "2   9.3255    7.75    9.62     8.90     8.80  \n",
       "3  12.1000   10.11    9.06     7.94     7.22  \n",
       "4  12.2500   10.40    9.16     8.03     7.32  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('CAF357_cleaned_xgb_complete.csv')\n",
    "data['Location'] = label_encoder.inverse_transform(data['Location'])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data[['Year', 'Month', 'Day']]).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "data = data.drop(columns=['Day', 'Month', 'Year'])\n",
    "\n",
    "correct_column_order = ['Location', 'Date', 'VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm', \n",
    "                        'T_30cm', 'T_60cm', 'T_90cm', 'T_120cm', 'T_150cm']\n",
    "\n",
    "data = data[correct_column_order]\n",
    "\n",
    "data.to_csv('CAF357_final_cleaned_xgb_complete.csv', index=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing v2.0 w/ MinMax Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1293, 9), Validation: (646, 9), Test: (647, 9)\n",
      "Training set size: 1293\n",
      "Validation set size: 646\n",
      "Testing set size: 647\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('CAF357_final_cleaned_xgb_complete.csv')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "features = data.drop(columns=['Date', 'Location', 'T_90cm'])\n",
    "targets = data['T_90cm']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, targets, test_size=0.5, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNN: {'n_neighbors': 2, 'p': 1, 'weights': 'distance'}\n",
      "KNN - Validation Metrics:\n",
      "MSE: 0.0248744978309027\n",
      "RMSE: 0.15771651096477723\n",
      "MAE: 0.08950204287366521\n",
      "R2: 0.998531222485488\n",
      "FSD: 0.03832463325998032\n",
      "Similarity: 0.9998886742622205\n",
      "\n",
      "KNN - Test Metrics:\n",
      "MSE: 0.019284507492682255\n",
      "RMSE: 0.13886866994640026\n",
      "MAE: 0.07971438930244491\n",
      "R2: 0.9989169734828409\n",
      "FSD: 0.032909368227893684\n",
      "Similarity: 0.9999093637244467\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define and fit the model with GridSearchCV\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [2, 3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for KNN: {grid_knn.best_params_}\")\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_knn = grid_knn.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_knn = mean_squared_error(y_val, y_val_pred_knn)\n",
    "rmse_knn = np.sqrt(mse_knn)  # Root Mean Squared Error\n",
    "mae_knn = mean_absolute_error(y_val, y_val_pred_knn)  # Mean Absolute Error\n",
    "r2_knn = r2_score(y_val, y_val_pred_knn)\n",
    "fsd_knn = rmse_knn / np.std(y_val)  # Fraction of Standard Deviation\n",
    "similarity_knn = np.dot(y_val, y_val_pred_knn) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_knn, y_val_pred_knn)))  # Cosine similarity\n",
    "\n",
    "print(f\"KNN - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_knn}\")\n",
    "print(f\"RMSE: {rmse_knn}\")\n",
    "print(f\"MAE: {mae_knn}\")\n",
    "print(f\"R2: {r2_knn}\")\n",
    "print(f\"FSD: {fsd_knn}\")\n",
    "print(f\"Similarity: {similarity_knn}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_knn = grid_knn.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_knn_test = mean_squared_error(y_test, y_test_pred_knn)\n",
    "rmse_knn_test = np.sqrt(mse_knn_test)\n",
    "mae_knn_test = mean_absolute_error(y_test, y_test_pred_knn)\n",
    "r2_knn_test = r2_score(y_test, y_test_pred_knn)\n",
    "fsd_knn_test = rmse_knn_test / np.std(y_test)\n",
    "similarity_knn_test = np.dot(y_test, y_test_pred_knn) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_knn, y_test_pred_knn)))\n",
    "\n",
    "print(f\"\\nKNN - Test Metrics:\")\n",
    "print(f\"MSE: {mse_knn_test}\")\n",
    "print(f\"RMSE: {rmse_knn_test}\")\n",
    "print(f\"MAE: {mae_knn_test}\")\n",
    "print(f\"R2: {r2_knn_test}\")\n",
    "print(f\"FSD: {fsd_knn_test}\")\n",
    "print(f\"Similarity: {similarity_knn_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Validation Metrics:\n",
      "MSE: 1.0189625858169293\n",
      "RMSE: 1.0094367666262851\n",
      "MAE: 0.5950706992279273\n",
      "R2: 0.9398327819781122\n",
      "FSD: 0.24529006914648596\n",
      "Similarity: 0.9954218211549597\n",
      "\n",
      "Linear Regression - Test Metrics:\n",
      "MSE: 0.8985179816001905\n",
      "RMSE: 0.9479018839522318\n",
      "MAE: 0.5999679448748754\n",
      "R2: 0.9495388305567805\n",
      "FSD: 0.22463563707306014\n",
      "Similarity: 0.9957726685619407\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Fit the model (assuming this part remains the same)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_lr = lr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_lr = mean_squared_error(y_val, y_val_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)  # Root Mean Squared Error\n",
    "mae_lr = mean_absolute_error(y_val, y_val_pred_lr)  # Mean Absolute Error\n",
    "r2_lr = r2_score(y_val, y_val_pred_lr)\n",
    "\n",
    "# FSD (Fraction of Standard Deviation) - ratio of RMSE to standard deviation of actual values\n",
    "fsd_lr = rmse_lr / np.std(y_val)\n",
    "\n",
    "# Similarity (cosine similarity between actual and predicted values)\n",
    "similarity_lr = np.dot(y_val, y_val_pred_lr) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_lr, y_val_pred_lr)))\n",
    "\n",
    "print(f\"Linear Regression - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_lr}\")\n",
    "print(f\"RMSE: {rmse_lr}\")\n",
    "print(f\"MAE: {mae_lr}\")\n",
    "print(f\"R2: {r2_lr}\")\n",
    "print(f\"FSD: {fsd_lr}\")\n",
    "print(f\"Similarity: {similarity_lr}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_lr_test = mean_squared_error(y_test, y_test_pred_lr)\n",
    "rmse_lr_test = np.sqrt(mse_lr_test)\n",
    "mae_lr_test = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "r2_lr_test = r2_score(y_test, y_test_pred_lr)\n",
    "fsd_lr_test = rmse_lr_test / np.std(y_test)\n",
    "similarity_lr_test = np.dot(y_test, y_test_pred_lr) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_lr, y_test_pred_lr)))\n",
    "\n",
    "print(f\"\\nLinear Regression - Test Metrics:\")\n",
    "print(f\"MSE: {mse_lr_test}\")\n",
    "print(f\"RMSE: {rmse_lr_test}\")\n",
    "print(f\"MAE: {mae_lr_test}\")\n",
    "print(f\"R2: {r2_lr_test}\")\n",
    "print(f\"FSD: {fsd_lr_test}\")\n",
    "print(f\"Similarity: {similarity_lr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM /w GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM - Validation Metrics:\n",
      "MSE: 0.15092768861128875\n",
      "RMSE: 0.3884941294425036\n",
      "MAE: 0.16417650790402144\n",
      "R2: 0.9910880936428744\n",
      "FSD: 0.09440289379635342\n",
      "Similarity: 0.999323430002695\n",
      "\n",
      "SVM - Test Metrics:\n",
      "MSE: 0.1679796614307954\n",
      "RMSE: 0.4098532193734672\n",
      "MAE: 0.1777571461960565\n",
      "R2: 0.990566187508704\n",
      "FSD: 0.09712781522970694\n",
      "Similarity: 0.9992108727746722\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define and fit the model with GridSearchCV\n",
    "svr = SVR()\n",
    "\n",
    "param_grid_svr = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_svr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for SVM: {grid_svr.best_params_}\")\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_svr = grid_svr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_svr = mean_squared_error(y_val, y_val_pred_svr)\n",
    "rmse_svr = np.sqrt(mse_svr)  # Root Mean Squared Error\n",
    "mae_svr = mean_absolute_error(y_val, y_val_pred_svr)  # Mean Absolute Error\n",
    "r2_svr = r2_score(y_val, y_val_pred_svr)\n",
    "fsd_svr = rmse_svr / np.std(y_val)  # Fraction of Standard Deviation\n",
    "similarity_svr = np.dot(y_val, y_val_pred_svr) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_svr, y_val_pred_svr)))  # Cosine similarity\n",
    "\n",
    "print(f\"SVM - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_svr}\")\n",
    "print(f\"RMSE: {rmse_svr}\")\n",
    "print(f\"MAE: {mae_svr}\")\n",
    "print(f\"R2: {r2_svr}\")\n",
    "print(f\"FSD: {fsd_svr}\")\n",
    "print(f\"Similarity: {similarity_svr}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_svr = grid_svr.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_svr_test = mean_squared_error(y_test, y_test_pred_svr)\n",
    "rmse_svr_test = np.sqrt(mse_svr_test)\n",
    "mae_svr_test = mean_absolute_error(y_test, y_test_pred_svr)\n",
    "r2_svr_test = r2_score(y_test, y_test_pred_svr)\n",
    "fsd_svr_test = rmse_svr_test / np.std(y_test)\n",
    "similarity_svr_test = np.dot(y_test, y_test_pred_svr) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_svr, y_test_pred_svr)))\n",
    "\n",
    "print(f\"\\nSVM - Test Metrics:\")\n",
    "print(f\"MSE: {mse_svr_test}\")\n",
    "print(f\"RMSE: {rmse_svr_test}\")\n",
    "print(f\"MAE: {mae_svr_test}\")\n",
    "print(f\"R2: {r2_svr_test}\")\n",
    "print(f\"FSD: {fsd_svr_test}\")\n",
    "print(f\"Similarity: {similarity_svr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest /w GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Random Forest - Validation MSE: 0.06070232561087914, R2: 0.9964156779549102\n",
      "Random Forest - Test MSE: 0.0333161724878445, R2: 0.9981289489364313\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {grid_rf.best_params_}\")\n",
    "\n",
    "y_val_pred_rf = grid_rf.predict(X_val)\n",
    "\n",
    "mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "print(f\"Random Forest - Validation MSE: {mse_rf}, R2: {r2_rf}\")\n",
    "\n",
    "y_test_pred_rf = grid_rf.predict(X_test)\n",
    "\n",
    "mse_rf_test = mean_squared_error(y_test, y_test_pred_rf)\n",
    "r2_rf_test = r2_score(y_test, y_test_pred_rf)\n",
    "print(f\"Random Forest - Test MSE: {mse_rf_test}, R2: {r2_rf_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n",
      "XGBoost - Validation Metrics:\n",
      "MSE: 0.06689785105413473\n",
      "RMSE: 0.25864618894183367\n",
      "MAE: 0.12059782110279191\n",
      "R2: 0.9960498475158998\n",
      "FSD: 0.06285023853654176\n",
      "Similarity: 0.9996999977390341\n",
      "\n",
      "XGBoost - Test Metrics:\n",
      "MSE: 0.0737005952284224\n",
      "RMSE: 0.2714785354837881\n",
      "MAE: 0.11735448414538709\n",
      "R2: 0.9958609417952168\n",
      "FSD: 0.06433551278091487\n",
      "Similarity: 0.9996540585659551\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define and fit the model with GridSearchCV\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {grid_xgb.best_params_}\")\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_xgb = grid_xgb.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_xgb = mean_squared_error(y_val, y_val_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)  # Root Mean Squared Error\n",
    "mae_xgb = mean_absolute_error(y_val, y_val_pred_xgb)  # Mean Absolute Error\n",
    "r2_xgb = r2_score(y_val, y_val_pred_xgb)\n",
    "fsd_xgb = rmse_xgb / np.std(y_val)  # Fraction of Standard Deviation\n",
    "similarity_xgb = np.dot(y_val, y_val_pred_xgb) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_xgb, y_val_pred_xgb)))  # Cosine similarity\n",
    "\n",
    "print(f\"XGBoost - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_xgb}\")\n",
    "print(f\"RMSE: {rmse_xgb}\")\n",
    "print(f\"MAE: {mae_xgb}\")\n",
    "print(f\"R2: {r2_xgb}\")\n",
    "print(f\"FSD: {fsd_xgb}\")\n",
    "print(f\"Similarity: {similarity_xgb}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_xgb = grid_xgb.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_xgb_test = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "rmse_xgb_test = np.sqrt(mse_xgb_test)\n",
    "mae_xgb_test = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "r2_xgb_test = r2_score(y_test, y_test_pred_xgb)\n",
    "fsd_xgb_test = rmse_xgb_test / np.std(y_test)\n",
    "similarity_xgb_test = np.dot(y_test, y_test_pred_xgb) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_xgb, y_test_pred_xgb)))\n",
    "\n",
    "print(f\"\\nXGBoost - Test Metrics:\")\n",
    "print(f\"MSE: {mse_xgb_test}\")\n",
    "print(f\"RMSE: {rmse_xgb_test}\")\n",
    "print(f\"MAE: {mae_xgb_test}\")\n",
    "print(f\"R2: {r2_xgb_test}\")\n",
    "print(f\"FSD: {fsd_xgb_test}\")\n",
    "print(f\"Similarity: {similarity_xgb_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 20, 'n_estimators': 300, 'num_leaves': 31}\n",
      "LightGBM - Validation Metrics:\n",
      "MSE: 0.058260417917071504\n",
      "RMSE: 0.24137194931696496\n",
      "MAE: 0.1225680874896391\n",
      "R2: 0.9965598665587388\n",
      "FSD: 0.05865265076073835\n",
      "Similarity: 0.9997388007043301\n",
      "\n",
      "LightGBM - Test Metrics:\n",
      "MSE: 0.04755354632493866\n",
      "RMSE: 0.21806775627070285\n",
      "MAE: 0.11865524474482415\n",
      "R2: 0.9973293716899742\n",
      "FSD: 0.051678122160405696\n",
      "Similarity: 0.9997762551256572\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define and fit the model with GridSearchCV\n",
    "lgbm = LGBMRegressor(random_state=42, verbosity=-1)\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'num_leaves': [31, 50, 100]\n",
    "}\n",
    "\n",
    "grid_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, scoring='neg_mean_squared_error', verbose=0)\n",
    "grid_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for LightGBM: {grid_lgbm.best_params_}\")\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_lgbm = grid_lgbm.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_lgbm = mean_squared_error(y_val, y_val_pred_lgbm)\n",
    "rmse_lgbm = np.sqrt(mse_lgbm)  # Root Mean Squared Error\n",
    "mae_lgbm = mean_absolute_error(y_val, y_val_pred_lgbm)  # Mean Absolute Error\n",
    "r2_lgbm = r2_score(y_val, y_val_pred_lgbm)\n",
    "fsd_lgbm = rmse_lgbm / np.std(y_val)  # Fraction of Standard Deviation\n",
    "similarity_lgbm = np.dot(y_val, y_val_pred_lgbm) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_lgbm, y_val_pred_lgbm)))  # Cosine similarity\n",
    "\n",
    "print(f\"LightGBM - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_lgbm}\")\n",
    "print(f\"RMSE: {rmse_lgbm}\")\n",
    "print(f\"MAE: {mae_lgbm}\")\n",
    "print(f\"R2: {r2_lgbm}\")\n",
    "print(f\"FSD: {fsd_lgbm}\")\n",
    "print(f\"Similarity: {similarity_lgbm}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_lgbm = grid_lgbm.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_lgbm_test = mean_squared_error(y_test, y_test_pred_lgbm)\n",
    "rmse_lgbm_test = np.sqrt(mse_lgbm_test)\n",
    "mae_lgbm_test = mean_absolute_error(y_test, y_test_pred_lgbm)\n",
    "r2_lgbm_test = r2_score(y_test, y_test_pred_lgbm)\n",
    "fsd_lgbm_test = rmse_lgbm_test / np.std(y_test)\n",
    "similarity_lgbm_test = np.dot(y_test, y_test_pred_lgbm) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_lgbm, y_test_pred_lgbm)))\n",
    "\n",
    "print(f\"\\nLightGBM - Test Metrics:\")\n",
    "print(f\"MSE: {mse_lgbm_test}\")\n",
    "print(f\"RMSE: {rmse_lgbm_test}\")\n",
    "print(f\"MAE: {mae_lgbm_test}\")\n",
    "print(f\"R2: {r2_lgbm_test}\")\n",
    "print(f\"FSD: {fsd_lgbm_test}\")\n",
    "print(f\"Similarity: {similarity_lgbm_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1293, 9), Validation: (646, 9), Test: (647, 9)\n",
      "Training set size: 1293\n",
      "Validation set size: 646\n",
      "Testing set size: 647\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('CAF357_final_cleaned_xgb_complete.csv')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "features = data.drop(columns=['Date', 'Location', 'VW_90cm'])\n",
    "targets = data['VW_90cm']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, targets, test_size=0.5, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Validation MSE: 0.0010955143156734088, R2: 0.7954319190204371\n",
      "Linear Regression - Test MSE: 0.0010850961059727065, R2: 0.7982046601561356\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_lr = lr.predict(X_val)\n",
    "\n",
    "mse_lr = mean_squared_error(y_val, y_val_pred_lr)\n",
    "r2_lr = r2_score(y_val, y_val_pred_lr)\n",
    "print(f\"Linear Regression - Validation MSE: {mse_lr}, R2: {r2_lr}\")\n",
    "\n",
    "y_test_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mse_lr_test = mean_squared_error(y_test, y_test_pred_lr)\n",
    "r2_lr_test = r2_score(y_test, y_test_pred_lr)\n",
    "print(f\"Linear Regression - Test MSE: {mse_lr_test}, R2: {r2_lr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for KNN: {'n_neighbors': 1, 'p': 1, 'weights': 'distance'}\n",
      "KNN - Validation MSE: 3.618266253869968e-05, R2: 0.9932435224858536\n",
      "KNN - Test MSE: 7.056414219474498e-05, R2: 0.9868771853694797\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [1, 2, 3, 5, 7, 9],\n",
    "    'weights': [ 'distance'],\n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for KNN: {grid_knn.best_params_}\")\n",
    "\n",
    "y_val_pred_knn = grid_knn.predict(X_val)\n",
    "\n",
    "mse_knn = mean_squared_error(y_val, y_val_pred_knn)\n",
    "r2_knn = r2_score(y_val, y_val_pred_knn)\n",
    "print(f\"KNN - Validation MSE: {mse_knn}, R2: {r2_knn}\")\n",
    "\n",
    "y_test_pred_knn = grid_knn.predict(X_test)\n",
    "\n",
    "mse_knn_test = mean_squared_error(y_test, y_test_pred_knn)\n",
    "r2_knn_test = r2_score(y_test, y_test_pred_knn)\n",
    "print(f\"KNN - Test MSE: {mse_knn_test}, R2: {r2_knn_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'C': 0.01, 'degree': 2, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "SVM - Validation MSE: 0.005903801083591329, R2: -0.10243128809590685\n",
      "SVM - Test MSE: 0.005862129443585779, R2: -0.09018030455163273\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "param_grid_svr = {\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
    "    'degree': [2, 3, 4]  # For 'poly' kernel\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_svr.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best parameters for SVM: {grid_svr.best_params_}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_svr = grid_svr.predict(X_val)\n",
    "mse_svr = mean_squared_error(y_val, y_val_pred_svr)\n",
    "r2_svr = r2_score(y_val, y_val_pred_svr)\n",
    "print(f\"SVM - Validation MSE: {mse_svr}, R2: {r2_svr}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_svr = grid_svr.predict(X_test)\n",
    "mse_svr_test = mean_squared_error(y_test, y_test_pred_svr)\n",
    "r2_svr_test = r2_score(y_test, y_test_pred_svr)\n",
    "print(f\"SVM - Test MSE: {mse_svr_test}, R2: {r2_svr_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Random Forest - Validation Metrics:\n",
      "MSE: 5.5656463576593234e-05\n",
      "RMSE: 0.007460325969861722\n",
      "MAE: 0.0026274604818573044\n",
      "R2: 0.9896071317507394\n",
      "FSD: 0.10194541799051388\n",
      "Similarity: 0.9997395204629812\n",
      "\n",
      "Random Forest - Test Metrics:\n",
      "MSE: 5.6394427790783885e-05\n",
      "RMSE: 0.007509622346748463\n",
      "MAE: 0.002419297829790419\n",
      "R2: 0.9895123273793891\n",
      "FSD: 0.10240933854200476\n",
      "Similarity: 0.9997312420119646\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {grid_rf.best_params_}\")\n",
    "\n",
    "# Validation predictions\n",
    "y_val_pred_rf = grid_rf.predict(X_val)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)  # Root Mean Squared Error\n",
    "mae_rf = mean_absolute_error(y_val, y_val_pred_rf)  # Mean Absolute Error\n",
    "r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "fsd_rf = rmse_rf / np.std(y_val)  # Fraction of Standard Deviation\n",
    "similarity_rf = np.dot(y_val, y_val_pred_rf) / (np.sqrt(np.dot(y_val, y_val)) * np.sqrt(np.dot(y_val_pred_rf, y_val_pred_rf)))  # Cosine similarity\n",
    "\n",
    "print(f\"Random Forest - Validation Metrics:\")\n",
    "print(f\"MSE: {mse_rf}\")\n",
    "print(f\"RMSE: {rmse_rf}\")\n",
    "print(f\"MAE: {mae_rf}\")\n",
    "print(f\"R2: {r2_rf}\")\n",
    "print(f\"FSD: {fsd_rf}\")\n",
    "print(f\"Similarity: {similarity_rf}\")\n",
    "\n",
    "# Test predictions\n",
    "y_test_pred_rf = grid_rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "mse_rf_test = mean_squared_error(y_test, y_test_pred_rf)\n",
    "rmse_rf_test = np.sqrt(mse_rf_test)\n",
    "mae_rf_test = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "r2_rf_test = r2_score(y_test, y_test_pred_rf)\n",
    "fsd_rf_test = rmse_rf_test / np.std(y_test)\n",
    "similarity_rf_test = np.dot(y_test, y_test_pred_rf) / (np.sqrt(np.dot(y_test, y_test)) * np.sqrt(np.dot(y_test_pred_rf, y_test_pred_rf)))\n",
    "\n",
    "print(f\"\\nRandom Forest - Test Metrics:\")\n",
    "print(f\"MSE: {mse_rf_test}\")\n",
    "print(f\"RMSE: {rmse_rf_test}\")\n",
    "print(f\"MAE: {mae_rf_test}\")\n",
    "print(f\"R2: {r2_rf_test}\")\n",
    "print(f\"FSD: {fsd_rf_test}\")\n",
    "print(f\"Similarity: {similarity_rf_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB Regressor w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 100}\n",
      "XGBoost - Validation MSE: 0.0001432306256974724, R2: 0.9732541931974452\n",
      "XGBoost - Test MSE: 8.529929374520264e-05, R2: 0.9841368890045692\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5, 0.8]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {grid_xgb.best_params_}\")\n",
    "\n",
    "y_val_pred_xgb = grid_xgb.predict(X_val)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_val, y_val_pred_xgb)\n",
    "r2_xgb = r2_score(y_val, y_val_pred_xgb)\n",
    "print(f\"XGBoost - Validation MSE: {mse_xgb}, R2: {r2_xgb}\")\n",
    "\n",
    "y_test_pred_xgb = grid_xgb.predict(X_test)\n",
    "\n",
    "mse_xgb_test = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "r2_xgb_test = r2_score(y_test, y_test_pred_xgb)\n",
    "print(f\"XGBoost - Test MSE: {mse_xgb_test}, R2: {r2_xgb_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightBGM w/ GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 20, 'n_estimators': 300, 'num_leaves': 31}\n",
      "LightGBM - Validation MSE: 0.00010641384844701092, R2: 0.9801290805103959\n",
      "LightGBM - Test MSE: 9.150246362440958e-05, R2: 0.982983285404857\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(random_state=42, verbosity=-1)\n",
    "\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'num_leaves': [31, 50, 100]\n",
    "}\n",
    "\n",
    "grid_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, scoring='neg_mean_squared_error', verbose = 0)\n",
    "grid_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for LightGBM: {grid_lgbm.best_params_}\")\n",
    "\n",
    "y_val_pred_lgbm = grid_lgbm.predict(X_val)\n",
    "\n",
    "mse_lgbm = mean_squared_error(y_val, y_val_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_val, y_val_pred_lgbm)\n",
    "print(f\"LightGBM - Validation MSE: {mse_lgbm}, R2: {r2_lgbm}\")\n",
    "\n",
    "y_test_pred_lgbm = grid_lgbm.predict(X_test)\n",
    "\n",
    "mse_lgbm_test = mean_squared_error(y_test, y_test_pred_lgbm)\n",
    "r2_lgbm_test = r2_score(y_test, y_test_pred_lgbm)\n",
    "print(f\"LightGBM - Test MSE: {mse_lgbm_test}, R2: {r2_lgbm_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
